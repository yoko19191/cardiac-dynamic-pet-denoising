{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12caa77d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "\n",
    "saved `.h5` dataset will be standarlized and normalzied between (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050ae8a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from module.transforms import Standardize, Normalize, RandomFlip, RandomRotate, RandomRotate90, ElasticDeformation, AdditiveGaussianNoise, AdditivePoissonNoise, ToTensor\n",
    "from module.utils import calculate_metrics, display_image_in_detail, plot_2d_data, display_4d_image\n",
    "from module.datasets import load_4d_dicom, save_4d_dicom\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7f030a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded...PT_20p 150_120 OSEM int16 (shape:(24, 71, 192, 192); range:[-32767,32767]; mean:201.12048776036573; std:1412.4139804758772)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2aa897fed74eb2b1413b90e0f8fd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Time', max=23), IntSlider(value=0, description='Slice', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "data_dir = \"./dataset/TracPETperf-D2-PhantomExperiments2/PT_20p 150_120 OSEM\"\n",
    "\n",
    "#\n",
    "slices_np = load_4d_dicom(data_dir)\n",
    "print(f\"Loaded...{os.path.basename(data_dir)} {slices_np.dtype} (shape:{slices_np.shape}; range:[{np.min(slices_np)},{np.max(slices_np)}]; mean:{np.mean(slices_np)}; std:{np.std(slices_np)})\")\n",
    "display_4d_image(slices_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f9e86",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Option 1: Real-world Noise Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b9666e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "norm_slices = (slices_np - np.mean(slices_np)) / np.std(slices_np)\n",
    "\n",
    "min_val = np.min(norm_slices)\n",
    "max_val = np.max(norm_slices)\n",
    "\n",
    "norm_slices = (norm_slices - np.min(norm_slices)) / (np.max(norm_slices) - np.min(norm_slices))\n",
    "\n",
    "\n",
    "# nessary info for restored \n",
    "restore_info = {'mean': np.mean(slices_np).item(),\n",
    "             'std': np.std(slices_np).item(),\n",
    "             'min': min_val.item(),\n",
    "             'max': max_val.item()}\n",
    "\n",
    "\n",
    "print(restore_info)\n",
    "\n",
    "\n",
    "# save real noise dataset\n",
    "file_name = f\"{os.path.basename(data_dir)}_real\"\n",
    "with h5py.File(f'./dataset/preprocessed/{file_name}.h5', 'w') as f:\n",
    "    f.create_dataset('dataset', data=norm_slices)\n",
    "    f.create_dataset('restore_info', data=np.array(json.dumps(restore_info), dtype=h5py.string_dtype()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd40d8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Option 2: Synthetic Noise Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613a5a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Prepare gaussian nosie dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdfd7d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# demostrate\n",
    "norm_slices = (slices_np - np.mean(slices_np)) / np.std(slices_np)\n",
    "norm_slices = (norm_slices - np.min(norm_slices)) / (np.max(norm_slices) - np.min(norm_slices))\n",
    "\n",
    "#\n",
    "rs = np.random.RandomState(seed=80)\n",
    "\n",
    "\n",
    "clean_image = norm_slices[11, 38]\n",
    "noisy_image = AdditiveGaussianNoise(rs, scale=(0.009, 0.009), execution_probability=1.0)(norm_slices)[11, 38]\n",
    "\n",
    "\n",
    "#\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].set_axis_off()\n",
    "ax[0].imshow(clean_image, cmap='hot')\n",
    "ax[0].set_title(\"origin\")\n",
    "\n",
    "ax[1].set_axis_off()\n",
    "ax[1].imshow(noisy_image, cmap='hot')\n",
    "ax[1].set_title(\"noisy\")\n",
    "\n",
    "ax[2].set_axis_off()\n",
    "ax[2].imshow(noisy_image - clean_image, cmap='hot')\n",
    "ax[2].set_title(\"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0aca8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "norm_slices = (slices_np - np.mean(slices_np)) / np.std(slices_np)\n",
    "\n",
    "min_val = np.min(norm_slices)\n",
    "max_val = np.max(norm_slices)\n",
    "\n",
    "norm_slices = (norm_slices - np.min(norm_slices)) / (np.max(norm_slices) - np.min(norm_slices))\n",
    "\n",
    "\n",
    "# nessary info for restored \n",
    "restore_info = {'mean': np.mean(slices_np).item(),\n",
    "             'std': np.std(slices_np).item(),\n",
    "             'min': min_val.item(),\n",
    "             'max': max_val.item()}\n",
    "\n",
    "print(restore_info)\n",
    "\n",
    "gaussian_scales = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009]\n",
    "\n",
    "# save gussian noise dataset\n",
    "rs = np.random.RandomState(seed=80)\n",
    "for scale in tqdm(gaussian_scales, desc='Processing'):\n",
    "    noisy_slices = AdditiveGaussianNoise(rs, scale=(scale, scale), execution_probability=1.0)(norm_slices)\n",
    "    file_name = f\"{os.path.basename(data_dir)}_gauss_{scale}\"\n",
    "    with h5py.File(f'./dataset/preprocessed/{file_name}.h5', 'w') as f:\n",
    "        f.create_dataset('dataset', data=noisy_slices)\n",
    "        f.create_dataset('restore_info', data=np.array(json.dumps(restore_info), dtype=h5py.string_dtype()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0673e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Prepare poission noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63077c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# demostrate\n",
    "norm_slices = (slices_np - np.mean(slices_np)) / np.std(slices_np)\n",
    "#norm_slices = (norm_slices - np.min(norm_slices)) / (np.max(norm_slices) - np.min(norm_slices))\n",
    "\n",
    "print(f\"norm_slices {norm_slices.shape} range:{(np.min(norm_slices), np.max(norm_slices))} mean:{np.mean(norm_slices)} std:{np.std(norm_slices)}\")\n",
    "\n",
    "#\n",
    "rs = np.random.RandomState(seed=80)\n",
    "\n",
    "#\n",
    "clean_image = norm_slices[11, 38]\n",
    "noisy_image = AdditivePoissonNoise(rs, lam=(0.9, 0.9), execution_probability=1.0)(norm_slices)[11, 38]\n",
    "\n",
    "#\n",
    "print(f\"noisy_image {noisy_image.shape} range:{(np.min(noisy_image), np.max(noisy_image))} mean:{np.mean(noisy_image)} std:{np.std(noisy_image)}\")\n",
    "noisy_image = (noisy_image - np.min(noisy_image)) / (np.max(noisy_image) - np.min(noisy_image))\n",
    "print(f\"noisy_image {noisy_image.shape} range:{(np.min(noisy_image), np.max(noisy_image))} mean:{np.mean(noisy_image)} std:{np.std(noisy_image)}\")\n",
    "\n",
    "#\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].set_axis_off()\n",
    "ax[0].imshow(clean_image, cmap='hot')\n",
    "ax[0].set_title(\"origin\")\n",
    "\n",
    "ax[1].set_axis_off()\n",
    "ax[1].imshow(noisy_image, cmap='hot')\n",
    "ax[1].set_title(\"noisy\")\n",
    "\n",
    "ax[2].set_axis_off()\n",
    "ax[2].imshow(noisy_image - clean_image, cmap='hot')\n",
    "ax[2].set_title(\"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25c357",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "norm_slices = (slices_np - np.mean(slices_np)) / np.std(slices_np)\n",
    "\n",
    "min_val = np.min(norm_slices)\n",
    "max_val = np.max(norm_slices)\n",
    "\n",
    "#norm_slices = (norm_slices - np.min(norm_slices)) / (np.max(norm_slices) - np.min(norm_slices))\n",
    "\n",
    "\n",
    "# nessary info for restored \n",
    "restore_info = {'mean': np.mean(slices_np).item(),\n",
    "             'std': np.std(slices_np).item(),\n",
    "             'min': min_val.item(),\n",
    "             'max': max_val.item()}\n",
    "\n",
    "print(restore_info)\n",
    "\n",
    "poission_lams = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "#poission_lams = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]\n",
    "\n",
    "# save gussian noise dataset\n",
    "rs = np.random.RandomState(seed=80)\n",
    "for lam in tqdm(poission_lams, desc='Processing'):\n",
    "    noisy_slices = AdditivePoissonNoise(rs, lam=(lam, lam), execution_probability=1.0)(norm_slices)\n",
    "    noisy_slices = (noisy_slices - np.min(noisy_slices)) / (np.max(noisy_slices)-np.min(noisy_slices))\n",
    "    file_name = f\"{os.path.basename(data_dir)}_poiss_{lam}\"\n",
    "    with h5py.File(f'./dataset/preprocessed/{file_name}.h5', 'w') as f:\n",
    "        f.create_dataset('dataset', data=noisy_slices)\n",
    "        f.create_dataset('restore_info', data=np.array(json.dumps(restore_info), dtype=h5py.string_dtype()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb15dda",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Option 3 : Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115eae14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "patient_dir = \"./dataset/TracPETperf-D2-PhantomExperiments2/PT_80p 250_50 OSEM\"\n",
    "\n",
    "slices_np = load_4d_dicom(patient_dir)\n",
    "\n",
    "print(f\"Loaded...{slices_np.dtype} (shape:{slices_np.shape}; range:[{np.min(slices_np)},{np.max(slices_np)}]; mean:{np.mean(slices_np)}; std:{np.std(slices_np)})\")\n",
    "\n",
    "display_4d_image(slices_np)\n",
    "\n",
    "#\n",
    "norm_slices = (slices_np - np.mean(slices_np)) / np.std(slices_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb61f2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_augmentation(norm_slices, seeds):\n",
    "    batch_all = []\n",
    "\n",
    "    # Adding original data at 1st batch\n",
    "    time_all = []\n",
    "    for time_frame in slices_np:\n",
    "        #aug = Standardize()(time_frame)\n",
    "        #aug = Normalize(min_value=0, max_value=1)(aug)\n",
    "        #aug = ToTensor(expand_dims=True)(aug)\n",
    "        #aug = ToTensor(expand_dims=True)(time_frame)\n",
    "        time_all.append(time_frame)\n",
    "\n",
    "    #batch_all.append(torch.stack(time_all).unsqueeze(0))\n",
    "    batch_all.append(np.array(time_all))\n",
    "\n",
    "    # Apply data augmented\n",
    "    for seed in tqdm(seeds, desc='Processing'):\n",
    "        time_all = []\n",
    "        for time_frame in slices_np:\n",
    "            rs = np.random.RandomState(seed)\n",
    "            \"\"\"<<<start...apply 3D transform on each time frame>>>\"\"\"\n",
    "            #aug = RandomRotate(rs, angle_spectrum=30, axes=[(1, 2)] ,mode='reflect')(time_frame) # this may cause artifact \n",
    "            aug = RandomRotate90(rs)(time_frame)\n",
    "            aug = ElasticDeformation(rs, 3, alpha=20, sigma=3, execution_probability=1.0)(aug)\n",
    "            #aug = Standardize()(aug)\n",
    "            #aug = AdditiveGaussianNoise(rs, scale=(0.0, 1.0), execution_probability=1.0)(aug)\n",
    "            #aug = AdditivePoissonNoise(rs, lam=(0, 1.0), execution_probability=1.0)(aug)\n",
    "            #aug = Normalize(min_value=0, max_value=1)(aug)\n",
    "            #aug = ToTensor(expand_dims=True)(aug)\n",
    "            #print(f\"aug : {aug.shape} range:[{aug.min()}, {aug.max()}] mean:{aug.mean()} std:{aug.std()}\")\n",
    "            time_all.append(aug)\n",
    "            \"\"\"<<<end....apply 3D transform on each time frame>>>\"\"\"\n",
    "        #time_all = torch.stack(time_all).unsqueeze(0)\n",
    "        #print(f\"tima_all : {time_all.shape} range:[{time_all.min()}, {time_all.max()}] mean:{time_all.mean()} std:{time_all.std()}\")\n",
    "        #batch_all.append(time_all)\n",
    "        batch_all.append(np.array(time_all))\n",
    "\n",
    "    #batch_all = torch.cat(batch_all, dim=0)\n",
    "    batch_all = np.array(batch_all)\n",
    "\n",
    "    # save tensor as (N, T, C, Z, X, Y)\n",
    "    \n",
    "    return batch_all\n",
    "\n",
    "\n",
    "#\n",
    "task_name = f\"{os.path.basename(patient_dir)}_augmented\"\n",
    "#task_name = \"PT_20p 150_120_augmented_combined\"\n",
    "print(f\"task_name: {task_name}\")\n",
    "\n",
    "#\n",
    "batch_num = 10 # number of batch data augmented\n",
    "\n",
    "# make augmentation reproducible\n",
    "np.random.seed(0)\n",
    "seeds = [np.random.randint(1, 1000) for _ in range(batch_num-1)]\n",
    "print(f\"seeds: {seeds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6507c1f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_all = process_augmentation(slices_np, seeds)\n",
    "#batch_all2 = process_transform(slices_np2, seeds)\n",
    "\n",
    "#print(f\"Process complete...{batch_all.dtype} (shape:{batch_all.shape}; range:[{batch_all.min()},{batch_all.max()}]; mean:{batch_all.mean()}; std:{batch_all.std()})\")\n",
    "print(f\"Process complete...{batch_all.dtype} (shape:{batch_all.shape}; range:[{np.min(batch_all)},{np.max(batch_all)}]; mean:{np.mean(batch_all)}; std:{np.std(batch_all)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a915dd6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display augmented data\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
    "ax = axes.ravel()\n",
    "\n",
    "#\n",
    "time_idx = 11\n",
    "depth_idx = 38\n",
    "for i in range(9):\n",
    "    ax[i].set_axis_off()\n",
    "    #image = batch_all[i, time_idx, 0, depth_idx].numpy()\n",
    "    image = batch_all[i, time_idx, depth_idx]\n",
    "    ax[i].imshow(image, cmap='hot')\n",
    "    print(f\"{image.shape} dtype:{image.dtype} range:({np.min(image)}，{np.max(image)}) mean:{np.mean(image)} std:{np.std(image)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a705e7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save raw data as h5\n",
    "with h5py.File(f'./dataset/{task_name}.h5', 'w') as f:\n",
    "    f.create_dataset('dataset', data=batch_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb4441",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### test\n",
    "\n",
    "convert to tensor and apply standarlize and normalization, then calculate PSNR and SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799c787",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    tensor_data = torch.from_numpy(data).float()\n",
    "    mean_val = tensor_data.mean()\n",
    "    std_val = tensor_data.std()\n",
    "    normalized_data = (tensor_data - mean_val) / std_val\n",
    "    min_val = normalized_data.min()\n",
    "    max_val = normalized_data.max()\n",
    "    normalized_data = (normalized_data - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return normalized_data, (mean_val, std_val, min_val, max_val)\n",
    "\n",
    "\n",
    "def denormalize(normalized_data, mean_val, std_val, min_val, max_val):\n",
    "    denormalied_data = normalized_data * (max_val - min_val) + min_val\n",
    "    original_data = denormalied_data * std_val + mean_val\n",
    "    return original_data\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbff74",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load noisy data\n",
    "with h5py.File('./dataset/PT_20p 150_120 OSEM_augmented.h5', 'r') as f:\n",
    "    noisy_data = f['dataset'][...]\n",
    "\n",
    "#print(f\"Noisy data...{noisy_data.dtype} (shape:{noisy_data.shape}; range:[{noisy_data.min()},{noisy_data.max()}]; mean:{noisy_data.mean()}; std:{noisy_data.std()})\")\n",
    "print(f\"Noisy data...{noisy_data.dtype} (shape:{noisy_data.shape}; range:[{np.min(noisy_data)},{np.max(noisy_data)}]; mean:{np.mean(noisy_data)}); std:{np.std(noisy_data)}\")\n",
    "\n",
    "\n",
    "# load clean data \n",
    "with h5py.File('./dataset/PT_80p 250_50 OSEM_augmented.h5', 'r') as f:\n",
    "    clean_data = f['dataset'][...]\n",
    "    \n",
    "#print(f\"Clean data...{clean_data.dtype} (shape:{clean_data.shape}; range:[{clean_data.min()},{clean_data.max()}]; mean:{clean_data.mean()}; std:{clean_data.std()})\")\n",
    "print(f\"Clean_data...{clean_data.dtype} (shape:{clean_data.shape}; range:[{np.min(clean_data)},{np.max(clean_data)}]; mean:{np.mean(clean_data)}); std:{np.std(clean_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe89f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_psnr, all_ssim = [], []\n",
    "for clean_batch, noisy_batch in zip(clean_data, noisy_data):\n",
    "    batch_psnr, batch_ssim = compute_psnr_ssim_from_4d_ndarray(clean_batch, noisy_batch)\n",
    "    all_psnr.append(batch_psnr)\n",
    "    all_ssim.append(batch_ssim)\n",
    "    \n",
    "all_psnr, all_ssim = np.array(all_psnr), np.array(all_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765a987",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "psnr_mean = np.mean(all_psnr)\n",
    "psnr_std = np.std(all_psnr)\n",
    "psnr_argmax = np.unravel_index(np.argmax(all_psnr), all_psnr.shape)\n",
    "psnr_argmin = np.unravel_index(np.argmin(all_psnr), all_psnr.shape)\n",
    "ssim_mean = np.mean(all_ssim)\n",
    "ssim_std = np.std(all_ssim)\n",
    "ssim_argmax = np.unravel_index(np.argmax(all_ssim), all_ssim.shape)\n",
    "ssim_argmin = np.unravel_index(np.argmin(all_ssim), all_ssim.shape)\n",
    "\n",
    "print(f\"all_psnr: mean: {psnr_mean}; std:{psnr_std}; argmin:{psnr_argmin}{all_psnr[psnr_argmin]}; argmax:{psnr_argmax}{all_psnr[psnr_argmax]}\")\n",
    "print(f\"all_ssim: mean: {ssim_mean}; std:{ssim_std}; argmin:{ssim_argmin}{all_ssim[ssim_argmin]}; argmax:{ssim_argmax}{all_ssim[ssim_argmax]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce230eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_idx = 0\n",
    "plot_2d_data(all_psnr[batch_idx], \"PSNR(db)\")\n",
    "plot_2d_data(all_ssim[batch_idx], \"SSIM(%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9a8d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert ndarray into tensor, then normalized\n",
    "norm_clean_tensor, (orignal_clean_mean, original_clean_std, original_clean_min, original_clean_max) = process_data(clean_data)\n",
    "norm_noisy_tensor, (orignal_noisy_mean, original_noisy_std, original_noisy_min, original_noisy_max) = process_data(noisy_data)\n",
    "\n",
    "print(f\"norm_clean_tensor: {norm_clean_tensor.shape}\\n mean: {orignal_clean_mean} -> {norm_clean_tensor.mean()}\\n std: {original_clean_std} -> {norm_clean_tensor.std()}\\n min: {original_clean_min} -> {norm_clean_tensor.min()}\\n max: {original_clean_max} -> {norm_clean_tensor.max()}\")\n",
    "print(f\"norm_noisy_tensor: {norm_noisy_tensor.shape}\\n mean: {orignal_noisy_mean} -> {norm_noisy_tensor.mean()}\\n std: {original_noisy_std} -> {norm_noisy_tensor.std()}\\n min: {original_noisy_min} -> {norm_noisy_tensor.min()}\\n max: {original_noisy_max} -> {norm_noisy_tensor.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1719ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_idx = 0\n",
    "display_4d_image(norm_clean_tensor[batch_idx].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8ebe4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# denormalize data\n",
    "clean_data_rev = denormalize(norm_clean_tensor, orignal_clean_mean, original_clean_std, original_clean_max, original_clean_min)\n",
    "noisy_data_rev = denormalize(norm_noisy_tensor, orignal_noisy_mean, original_noisy_std, original_noisy_max, original_noisy_min)\n",
    "\n",
    "print(f\"clean_data_rev: {clean_data_rev.shape} mean:{clean_data_rev.mean()} std:{clean_data_rev.std()} min:{clean_data_rev.min()} max:{clean_data_rev.max()}\")\n",
    "print(f\"noisy_data_rev: {noisy_data_rev.shape} mean:{noisy_data_rev.mean()} std:{noisy_data_rev.std()} min:{noisy_data_rev.min()} max:{noisy_data_rev.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71389d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_4d_image(clean_data_rev[batch_idx].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
