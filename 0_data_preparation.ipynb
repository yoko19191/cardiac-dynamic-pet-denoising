{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12caa77d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050ae8a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from module.transforms import Standardize, Normalize, RandomFlip, RandomRotate, RandomRotate90, ElasticDeformation, AdditiveGaussianNoise, AdditivePoissonNoise, ToTensor\n",
    "from module.utils import calculate_metrics, display_image_in_detail, plot_2d_data, display_4d_image, timer_decorator\n",
    "from module.datasets import load_4d_dicom, save_4d_dicom\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f030a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "data_dir = \"./dataset/10_05_2021_PET_only/PT_20p 150_120 OSEM\"\n",
    "\n",
    "#\n",
    "real_data = load_4d_dicom(data_dir)\n",
    "print(f\"Loaded...{os.path.basename(data_dir)} {real_data.dtype} (shape:{real_data.shape}; range:[{np.min(real_data)},{np.max(real_data)}]; mean:{np.mean(real_data)}; std:{np.std(real_data)})\")\n",
    "display_4d_image(real_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f9e86",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Option 1: Real-world Noise Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b9666e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "norm_slices = (real_data - np.mean(real_data)) / np.std(real_data)\n",
    "\n",
    "min_val = np.min(norm_slices)\n",
    "max_val = np.max(norm_slices)\n",
    "\n",
    "norm_slices = (norm_slices - np.min(norm_slices)) / (np.max(norm_slices) - np.min(norm_slices))\n",
    "\n",
    "\n",
    "# nessary info for restored \n",
    "restore_info = {'mean': np.mean(real_data).item(),\n",
    "             'std': np.std(real_data).item(),\n",
    "             'min': min_val.item(),\n",
    "             'max': max_val.item()}\n",
    "\n",
    "\n",
    "print(restore_info)\n",
    "\n",
    "\n",
    "# save real noise dataset\n",
    "file_name = f\"{os.path.basename(data_dir)}_real\"\n",
    "with h5py.File(f'./dataset/preprocessed/{file_name}.h5', 'w') as f:\n",
    "    f.create_dataset('dataset', data=norm_slices)\n",
    "    f.create_dataset('restore_info', data=np.array(json.dumps(restore_info), dtype=h5py.string_dtype()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd40d8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Option 2: Synthetic Noise Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613a5a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Prepare gaussian nosie dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdfd7d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example\n",
    "norm_slices = (real_data - np.mean(real_data)) / np.std(real_data)\n",
    "norm_slices = (norm_slices - np.min(norm_slices)) / (np.max(norm_slices) - np.min(norm_slices))\n",
    "\n",
    "#\n",
    "rs = np.random.RandomState(seed=80)\n",
    "\n",
    "\n",
    "clean_image = norm_slices[11, 38]\n",
    "noisy_image = AdditiveGaussianNoise(rs, scale=(0.009, 0.009), execution_probability=1.0)(norm_slices)[11, 38]\n",
    "\n",
    "\n",
    "#\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].set_axis_off()\n",
    "ax[0].imshow(clean_image, cmap='hot')\n",
    "ax[0].set_title(\"origin\")\n",
    "\n",
    "ax[1].set_axis_off()\n",
    "ax[1].imshow(noisy_image, cmap='hot')\n",
    "ax[1].set_title(\"noisy\")\n",
    "\n",
    "ax[2].set_axis_off()\n",
    "ax[2].imshow(noisy_image - clean_image, cmap='hot')\n",
    "ax[2].set_title(\"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0aca8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "norm_slices = (real_data - np.mean(real_data)) / np.std(real_data)\n",
    "\n",
    "min_val = np.min(norm_slices)\n",
    "max_val = np.max(norm_slices)\n",
    "\n",
    "norm_slices = (norm_slices - np.min(norm_slices)) / (np.max(norm_slices) - np.min(norm_slices))\n",
    "\n",
    "\n",
    "# nessary info for restored \n",
    "restore_info = {'mean': np.mean(real_data).item(),\n",
    "             'std': np.std(real_data).item(),\n",
    "             'min': min_val.item(),\n",
    "             'max': max_val.item()}\n",
    "\n",
    "print(restore_info)\n",
    "\n",
    "gaussian_scales = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009]\n",
    "\n",
    "# save gussian noise dataset\n",
    "rs = np.random.RandomState(seed=80)\n",
    "for scale in tqdm(gaussian_scales, desc='Processing'):\n",
    "    noisy_slices = AdditiveGaussianNoise(rs, scale=(scale, scale), execution_probability=1.0)(norm_slices)\n",
    "    file_name = f\"{os.path.basename(data_dir)}_gauss_{scale}\"\n",
    "    with h5py.File(f'./dataset/preprocessed/{file_name}.h5', 'w') as f:\n",
    "        f.create_dataset('dataset', data=noisy_slices)\n",
    "        f.create_dataset('restore_info', data=np.array(json.dumps(restore_info), dtype=h5py.string_dtype()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0673e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Prepare poission noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63077c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example\n",
    "norm_slices = (real_data - np.mean(real_data)) / np.std(real_data)\n",
    "#norm_slices = (norm_slices - np.min(norm_slices)) / (np.max(norm_slices) - np.min(norm_slices))\n",
    "\n",
    "print(f\"norm_slices {norm_slices.shape} range:{(np.min(norm_slices), np.max(norm_slices))} mean:{np.mean(norm_slices)} std:{np.std(norm_slices)}\")\n",
    "\n",
    "#\n",
    "rs = np.random.RandomState(seed=80)\n",
    "\n",
    "#\n",
    "clean_image = norm_slices[11, 38]\n",
    "noisy_image = AdditivePoissonNoise(rs, lam=(0.9, 0.9), execution_probability=1.0)(norm_slices)[11, 38]\n",
    "\n",
    "#\n",
    "print(f\"noisy_image {noisy_image.shape} range:{(np.min(noisy_image), np.max(noisy_image))} mean:{np.mean(noisy_image)} std:{np.std(noisy_image)}\")\n",
    "noisy_image = (noisy_image - np.min(noisy_image)) / (np.max(noisy_image) - np.min(noisy_image))\n",
    "print(f\"noisy_image {noisy_image.shape} range:{(np.min(noisy_image), np.max(noisy_image))} mean:{np.mean(noisy_image)} std:{np.std(noisy_image)}\")\n",
    "\n",
    "#\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].set_axis_off()\n",
    "ax[0].imshow(clean_image, cmap='hot')\n",
    "ax[0].set_title(\"origin\")\n",
    "\n",
    "ax[1].set_axis_off()\n",
    "ax[1].imshow(noisy_image, cmap='hot')\n",
    "ax[1].set_title(\"noisy\")\n",
    "\n",
    "ax[2].set_axis_off()\n",
    "ax[2].imshow(noisy_image - clean_image, cmap='hot')\n",
    "ax[2].set_title(\"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25c357",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_slices = (real_data - np.mean(real_data)) / np.std(real_data)\n",
    "\n",
    "min_val = np.min(norm_slices)\n",
    "max_val = np.max(norm_slices)\n",
    "\n",
    "#norm_slices = (norm_slices - np.min(norm_slices)) / (np.max(norm_slices) - np.min(norm_slices))\n",
    "\n",
    "\n",
    "# nessary info for restored \n",
    "restore_info = {'mean': np.mean(real_data).item(),\n",
    "             'std': np.std(real_data).item(),\n",
    "             'min': min_val.item(),\n",
    "             'max': max_val.item()}\n",
    "\n",
    "print(restore_info)\n",
    "\n",
    "poission_lams = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "#poission_lams = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]\n",
    "\n",
    "# save gussian noise dataset\n",
    "rs = np.random.RandomState(seed=80)\n",
    "for lam in tqdm(poission_lams, desc='Processing'):\n",
    "    noisy_slices = AdditivePoissonNoise(rs, lam=(lam, lam), execution_probability=1.0)(norm_slices)\n",
    "    noisy_slices = (noisy_slices - np.min(noisy_slices)) / (np.max(noisy_slices)-np.min(noisy_slices))\n",
    "    file_name = f\"{os.path.basename(data_dir)}_poiss_{lam}\"\n",
    "    with h5py.File(f'./dataset/preprocessed/{file_name}.h5', 'w') as f:\n",
    "        f.create_dataset('dataset', data=noisy_slices)\n",
    "        f.create_dataset('restore_info', data=np.array(json.dumps(restore_info), dtype=h5py.string_dtype()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb15dda",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Option 3 : Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ab761",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10586aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_data: int16 (shape:(24, 71, 192, 192); range:[-32767,32767]; mean:201.12048776036573; std:1412.4139804758772)\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "data_dir = \"./dataset/10_05_2021_PET_only/PT_20p 150_120 OSEM\"\n",
    "\n",
    "real_data = load_4d_dicom(data_dir)\n",
    "\n",
    "print(f\"real_data: {real_data.dtype} (shape:{real_data.shape}; range:[{np.min(real_data)},{np.max(real_data)}]; mean:{np.mean(real_data)}; std:{np.std(real_data)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cbd67",
   "metadata": {},
   "source": [
    "### process data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d3589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_volume(volume, rs):\n",
    "    \"\"\"\n",
    "    Applies specific augmentations (random rotation and elastic deformation) to a 3D volume.\n",
    "\n",
    "    Args:\n",
    "        volume (numpy.ndarray): The input 3D volume.\n",
    "        rs (numpy.random.RandomState): The random state for ensuring consistency.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The augmented 3D volume.\n",
    "    \"\"\"\n",
    "    # random_roate\n",
    "    random_rotate90 = RandomRotate90(rs) \n",
    "    # elastic deformation\n",
    "    elastic_deformation = ElasticDeformation(rs, 3, alpha=20, sigma=3, execution_probability=1.0)\n",
    "    # \n",
    "    auged_volume = random_rotate90(volume)\n",
    "    auged_volume = elastic_deformation(auged_volume)\n",
    "    return auged_volume\n",
    "\n",
    "@timer_decorator\n",
    "def process_batch(data, batch_size, seed):\n",
    "    \"\"\"\n",
    "    Applies data augmentation to a given 4D dataset and returns the augmented data along with the original data.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The input data with shape (time, depth, height, width).\n",
    "        batch_size (int): The number of additional augmented batches required.\n",
    "        seed (int): The random seed for ensuring consistency in augmentation.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The augmented data with shape (batch_size+1, time, depth, height, width).\n",
    "    \"\"\"\n",
    "    \n",
    "    time, depth, height, width = data.shape\n",
    "    augmented_data = np.zeros((batch_size+1, time, depth, height, width), dtype=data.dtype)\n",
    "    augmented_data[0]=data # add original data in first batch \n",
    "    for batch in range(1, batch_size+1):\n",
    "        # Create a random state for this batch and use it consistently across the entire time sequence\n",
    "        for t in range(time):\n",
    "            rs = np.random.RandomState(seed + batch)\n",
    "            augmented_data[batch, t] = process_volume(data[t], rs)\n",
    "    \n",
    "    return augmented_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f801d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "batch_data = process_batch(real_data, batch_size=10, seed=42)\n",
    "print(f\"batch_data\\n dtype:{batch_data.dtype}\\n range:({np.min(batch_data)}, {np.max(batch_data)})\\n mean:{np.mean(batch_data)}\\n std_dev:{np.std(batch_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa1d5a",
   "metadata": {},
   "source": [
    "### adding synthetic noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628d913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_noise(batch_data, noise_type, scale, lam):\n",
    "    rs = np.random.RandomState(42)\n",
    "    \n",
    "    batch_data_float = batch_data.astype(np.float32)\n",
    "\n",
    "    # z-score\n",
    "    mean = batch_data_float.mean()\n",
    "    std_dev = batch_data_float.std()\n",
    "\n",
    "    z_scored_data = (batch_data_float - mean) / std_dev\n",
    "\n",
    "    # adding noise\n",
    "    if noise_type == 'gaussian':\n",
    "        add_gaussian_noise = AdditiveGaussianNoise(rs, scale=(scale, scale), execution_probability=1.0)\n",
    "        noised_data = add_gaussian_noise(z_scored_data)\n",
    "    elif noise_type == 'poisson':\n",
    "        add_poisson_noise = AdditivePoissonNoise(rs, lam=(lam, lam), execution_probability=1.0)\n",
    "        noised_data = add_poisson_noise(z_scored_data)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid noise type. Choose 'gaussian' or 'poisson'.\")\n",
    "\n",
    "    # normalize\n",
    "    normalized_data = (noised_data - noised_data.min()) / (noised_data.max() - noised_data.min())\n",
    "\n",
    "    #  restore information\n",
    "    restore_info = {\n",
    "        \"original_min\": batch_data_float.min(),\n",
    "        \"original_max\": batch_data_float.max(),\n",
    "        \"z_score_mean\": mean,\n",
    "        \"z_score_std_dev\": std_dev,\n",
    "        \"noise_min\": noised_data.min(),\n",
    "        \"noise_max\": noised_data.max()\n",
    "    }\n",
    "\n",
    "    return normalized_data, restore_info\n",
    "\n",
    "\n",
    "\n",
    "# exampple adding\n",
    "test_volume = batch_data[0, 11]\n",
    "\n",
    "display_image_in_detail(test_volume[38], title=\"before\")\n",
    "print(f\"dtype:{test_image.dtype}; range:({np.min(test_image)}, {np.max(test_image)}); mean:{np.mean(test_image)}; std:{np.std(test_image)}\")\n",
    "\n",
    "\"\"\"\n",
    "mean=0\n",
    "scale = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "lam = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\"\"\"\n",
    "noisy_volume, _ = add_noise(test_volume, noise_type=\"gaussian\", scale=0.15, lam=0.45)\n",
    "\n",
    "\n",
    "display_image_in_detail(noisy_volume[38], title=\"after\") \n",
    "print(f\"dtype:{noisy_image.dtype}; range:({np.min(noisy_image)}, {np.max(noisy_image)}); mean:{np.mean(noisy_image)}; std:{np.std(noisy_image)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97ab4e-b124-4069-9155-2b593520f655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "display_image_in_detail(batch_data[0, 11, 38], title=\"batch_data\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "gaussian\n",
    "scale_list = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "poisson\n",
    "lam_list = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "noise_type = \"gaussian\"\n",
    "# noise_type = \"poisson\"\n",
    "noise_level = 0.2\n",
    "\n",
    "#\n",
    "noisy_batch, restore_info = add_noise(batch_data, noise_type=noise_type, scale=noise_level, lam=noise_level)\n",
    "\n",
    "noisy_batch = noisy_batch.astype(np.float32)\n",
    "\n",
    "print(f\"noisy_batch\\n dtype:{noisy_batch.dtype}\\n range:({np.min(noisy_batch)}, {np.max(noisy_batch)})\\n mean:{np.mean(noisy_batch)}\\n std_dev:{np.std(noisy_batch)}\")\n",
    "display_image_in_detail(noisy_batch[0, 11, 38], title=\"noisy_batch\")\n",
    "\n",
    "print(restore_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c39ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### save as .h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591da646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = f\"{os.path.basename(data_dir)}_{noise_type}_{noise_level}_batch\"\n",
    "\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ffb71-e731-4fc8-b6cc-7da0d6e6ae9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "restore_info_standard_types = {key: float(value) for key, value in restore_info.items()}\n",
    "json_str = json.dumps(restore_info_standard_types)\n",
    "\n",
    "with h5py.File(f'./dataset/preprocessed/{file_name}.h5', 'w') as f:\n",
    "    f.create_dataset('dataset', data=noisy_batch)\n",
    "    f.create_dataset('restore_info', data=np.array(json_str, dtype=h5py.string_dtype()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05faa049",
   "metadata": {
    "tags": []
   },
   "source": [
    "### restore example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f41f1-5df5-4d27-b538-41ef0cc1df69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from module.datasets import restore_data\n",
    "\n",
    "restored_data = restore_data(noisy_batch, restore_info)\n",
    "print(f\"restore_data: {restored_data.dtype} shape:{restored_data.shape}; range:({np.min(restored_data)},{np.max(restored_data)}); mean:{np.mean(restored_data)}; std:{np.std(restored_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
